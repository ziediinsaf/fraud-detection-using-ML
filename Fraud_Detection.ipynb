{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Features\n",
    "\n",
    "- **21 features** capturing various aspects of a financial transaction\n",
    "- **Realistic structure** with numerical, categorical, and temporal data\n",
    "- **Binary fraud labels** (`0 = Not Fraud`, `1 = Fraud`)\n",
    "- **Useful for** anomaly detection, risk analysis, and security research\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Column Name                  | Description                                                         |\n",
    "| ---------------------------- | ------------------------------------------------------------------- |\n",
    "| Transaction_ID               | Unique identifier for each transaction                              |\n",
    "| User_ID                      | Unique identifier for the user                                      |\n",
    "| Transaction_Amount           | Amount of money involved in the transaction                         |\n",
    "| Transaction_Type             | Type of transaction (Online, In-Store, ATM, etc.)                   |\n",
    "| Timestamp                    | Date and time of the transaction                                    |\n",
    "| Account_Balance              | User's current account balance before the transaction               |\n",
    "| Device_Type                  | Type of device used (Mobile, Desktop, etc.)                         |\n",
    "| Location                     | Geographical location of the transaction                            |\n",
    "| Merchant_Category            | Type of merchant (Retail, Food, Travel, etc.)                       |\n",
    "| IP_Address_Flag              | Whether the IP address was flagged as suspicious (0 or 1)           |\n",
    "| Previous_Fraudulent_Activity | Number of past fraudulent activities by the user                    |\n",
    "| Daily_Transaction_Count      | Number of transactions made by the user that day                    |\n",
    "| Avg_Transaction_Amount_7d    | User's average transaction amount in the past 7 days                |\n",
    "| Failed_Transaction_Count_7d  | Count of failed transactions in the past 7 days                     |\n",
    "| Card_Type                    | Type of payment card used (Credit, Debit, Prepaid, etc.)            |\n",
    "| Card_Age                     | Age of the card in months                                           |\n",
    "| Transaction_Distance         | Distance between the user's usual location and transaction location |\n",
    "| Authentication_Method        | How the user authenticated (PIN, Biometric, etc.)                   |\n",
    "| Risk_Score                   | Fraud risk score computed for the transaction                       |\n",
    "| Is_Weekend                   | Whether the transaction occurred on a weekend (0 or 1)              |\n",
    "| Fraud_Label                  | Target variable (`0 = Not Fraud`, `1 = Fraud`)                      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import make_classification\n",
    "from xgboost import XGBClassifier\n",
    "import joblib\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data from CSV file to pandas dataframe\n",
    "file_path = 'synthetic_fraud_dataset.csv'\n",
    "\n",
    "df= pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing first five rows of the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing last five rows of the data\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#printing the number of rows and columns\n",
    "print(\"number of features in the Dataset:\",df.shape[1])\n",
    "print(\"number of instances in the Dataset:\",df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#printing the names of the columns\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting info about the Dataset\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Timestamp'] = pd.to_datetime(df['Timestamp'], errors='coerce')\n",
    "\n",
    "# Check result\n",
    "print(df['Timestamp'].head())\n",
    "print(df['Timestamp'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtaining Statistical Information About the Dataset\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking For Duplicate Rows In Dataset\n",
    "print('Number of Duplicated Rows :',df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking For Missing Values In Dataset\n",
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = df.select_dtypes(include='number').columns\n",
    "df.groupby('Fraud_Label')[numeric_cols].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = df.select_dtypes(include='object').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the occurrences of each category in the 'status' column\n",
    "status_counts = df['Fraud_Label'].value_counts()\n",
    "print(status_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 = Not Fraud\n",
    "# 1 = Fraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a pie plot\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.pie(status_counts, labels=status_counts.index, autopct='%1.2f%%', startangle=90, colors=['lightpink', 'lightgreen'])\n",
    "plt.title('Distribution of Parkinson\\'s Status')\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Data seems imbalanced**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the heatmap\n",
    "df_numeric=df.select_dtypes(include='number')\n",
    "fig, ax = plt.subplots(figsize=(20,20))  \n",
    "sns.heatmap(df_numeric.corr(),annot=True,ax=ax)\n",
    "ax.set_title('Correlation Heatmap of Dataset', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deleting non relevent columns\n",
    "df = df.drop(columns=['Transaction_ID', 'User_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "df['Hour'] = df['Timestamp'].dt.hour\n",
    "df['DayOfWeek'] = df['Timestamp'].dt.dayofweek\n",
    "df['Month'] = df['Timestamp'].dt.month\n",
    "df['Is_Weekend'] = df['DayOfWeek'] >= 5  # optional if not already present\n",
    "\n",
    "# Drop the raw Timestamp\n",
    "df = df.drop(columns=['Timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "categorical_cols = [\n",
    "    'Transaction_Type',\n",
    "    'Device_Type',\n",
    "    'Location',\n",
    "    'Merchant_Category',\n",
    "    'Card_Type',\n",
    "    'Authentication_Method'\n",
    "]\n",
    "\n",
    "# Initialize LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Apply Label Encoding to each categorical column\n",
    "for col in categorical_cols:\n",
    "    df[col] = le.fit_transform(df[col])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balancing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the occurrences of each category in the 'status' column\n",
    "status_counts = df['Fraud_Label'].value_counts()\n",
    "print(status_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating the features (X) and the target (y)\n",
    "X = df.drop(columns='Fraud_Label')\n",
    "y = df['Fraud_Label'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Feature Shape Before Balancing :', X.shape)\n",
    "print('Target Shape Before Balancing :', y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intialising SMOTE Object\n",
    "sm = SMOTE(random_state=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resampling The  Data\n",
    "X, y = sm.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Feature Shape After Balancing :', X.shape)\n",
    "print('Target Shape After Balancing :', y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the occurrences of each category in the 'status' column\n",
    "status_counts = y.value_counts()\n",
    "print(status_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the MinMaxScaler\n",
    "scaler = MinMaxScaler((-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the scaler to X (features only)\n",
    "X_features = scaler.fit_transform(X)\n",
    "y_labels =y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the dataset into traning and testing sets \n",
    "X_train , X_test , y_train , y_test = train_test_split(X_features, y_labels , test_size=0.2, random_state=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('X_train shape :', X_train.shape)\n",
    "print('X_test shape :', X_test.shape)\n",
    "print('y_train shape :', y_train.shape)\n",
    "print('y_test shape :', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing the logistic regression model\n",
    "lrmodel = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrmodel.fit(X_train, y_train)\n",
    "y_test_predlr = lrmodel.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classification report\n",
    "print(classification_report(y_test, y_test_predlr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_predlr = lrmodel.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating accuracy scores\n",
    "train_accuracy = accuracy_score(y_train, y_train_predlr)\n",
    "test_accuracy = accuracy_score(y_test, y_test_predlr)\n",
    "\n",
    "# Printing the accuracy scores\n",
    "print(f'Training Accuracy: {train_accuracy:.2f}')\n",
    "print(f'Test Accuracy: {test_accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix of Logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_test_predlr)\n",
    "\n",
    "# Creating a ConfusionMatrixDisplay object\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=lrmodel.classes_)\n",
    "\n",
    "# Plotting the confusion matrix\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix for Logistic Regression', y=1.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUC of Logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = lrmodel.predict_proba(X_test)[::,1]\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test, y_pred_proba)\n",
    "auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
    "plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic Curve')\n",
    "plt.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dumping Logistic Regression Model\n",
    "joblib.dump(lrmodel, 'lrmodel.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing the DecisionTreeClassifier model\n",
    "DTmodel=DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTmodel.fit(X_train , y_train)\n",
    "y_test_predDT = DTmodel.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification report\n",
    "print(classification_report(y_test, y_test_predDT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_predDT = DTmodel.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating accuracy scores\n",
    "train_accuracy = accuracy_score(y_train, y_train_predDT)\n",
    "test_accuracy = accuracy_score(y_test, y_test_predDT)\n",
    "\n",
    "# Printing the accuracy scores\n",
    "print(f'Training Accuracy: {train_accuracy:.2f}')\n",
    "print(f'Test Accuracy: {test_accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix for DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_test_predDT)\n",
    "\n",
    "# Creating a ConfusionMatrixDisplay object\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=DTmodel.classes_)\n",
    "\n",
    "# Plotting the confusion matrix\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix for DecisionTreeClassifier', y=1.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AUC of DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = DTmodel.predict_proba(X_test)[::,1]\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test, y_pred_proba)\n",
    "auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
    "plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic Curve')\n",
    "plt.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ks = 10\n",
    "mean_acc = []\n",
    "ConfustionMx = [];\n",
    "for n in range(2,Ks):\n",
    "    \n",
    "    #Train Model and Predict  \n",
    "    neigh = KNeighborsClassifier(n_neighbors = n).fit(X_train,y_train)\n",
    "    y_pred=neigh.predict(X_test)\n",
    "    mean_acc.append(metrics.accuracy_score(y_test, y_pred))  \n",
    "print('Neighbor Accuracy List')\n",
    "print(mean_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(2,Ks),mean_acc,'g')\n",
    "plt.ylabel('Accuracy ')\n",
    "plt.xlabel('Number of Neighbours (K)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing a KNN model\n",
    "knn_model=KNeighborsClassifier(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_predknn =knn_model.predict(X_test)\n",
    "y_train_predknn=knn_model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_test_predknn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating accuracy scores\n",
    "train_accuracy = accuracy_score(y_train, y_train_predknn)\n",
    "test_accuracy = accuracy_score(y_test, y_test_predknn)\n",
    "\n",
    "# Printing the accuracy scores\n",
    "print(f'Training Accuracy: {train_accuracy:.2f}')\n",
    "print(f'Test Accuracy: {test_accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix for KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the confusion matrix for KNN\n",
    "cm = confusion_matrix(y_test, y_test_predknn)\n",
    "\n",
    "# Creating a ConfusionMatrixDisplay object\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=knn_model.classes_)\n",
    "\n",
    "# Plotting the confusion matrix\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix for KNeighborsClassifier', y=1.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = knn_model.predict_proba(X_test)[::,1]\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test, y_pred_proba)\n",
    "auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
    "plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic Curve')\n",
    "plt.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dumping KNN Classifier\n",
    "joblib.dump(knn_model, 'knn_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing a Random Forest Classifier model\n",
    "rf_model=RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model.fit(X_train , y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_predrf=rf_model.predict(X_test)\n",
    "y_train_predrf=rf_model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_test_predrf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating accuracy scores\n",
    "training_accuracy=accuracy_score(y_train , y_train_predlr)\n",
    "test_accuracy=accuracy_score(y_test , y_test_predlr)\n",
    "\n",
    "# Plotting accuracy scores\n",
    "print(\"training accuracy: \",training_accuracy)\n",
    "print(\"test accuracy: \",test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the confusion matrix for Random Forest Classifier\n",
    "cm = confusion_matrix(y_test, y_test_predrf)\n",
    "\n",
    "# Creating a ConfusionMatrixDisplay object\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=rf_model.classes_)\n",
    "\n",
    "# Plotting the confusion matrix\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix for Random Forest Classifier', y=1.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = rf_model.predict_proba(X_test)[::,1]\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test, y_pred_proba)\n",
    "auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
    "plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic Curve')\n",
    "plt.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dumping Random Forest Classifier\n",
    "joblib.dump(rf_model, 'rf_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Parameter Dictionary\n",
    "param_dict = {'max_depth': range(4,8), 'eta' : [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "              'reg_lambda' : [0.8, 0.9, 1, 1.1, 1.2],\n",
    "              'random_state': [300, 600, 900]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGB = GridSearchCV(XGBClassifier(), param_grid = param_dict,\n",
    "                   scoring = 'f1', cv = 3, verbose = 1)\n",
    "XGB.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best Score :', XGB.best_score_)\n",
    "print('Best Parameters :', XGB.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Best Classifier From GridSearchCV\n",
    "XGB_model= XGB.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGB_model.fit(X_train , y_train)\n",
    "y_test_predx=XGB_model.predict(X_test)\n",
    "y_train_predx=XGB_model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_accuracy=accuracy_score(y_train , y_train_predx)\n",
    "test_accuracy=accuracy_score(y_test , y_test_predx)\n",
    "\n",
    "print(\"training accuracy: \",training_accuracy)\n",
    "print(\"test accuracy: \",test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_test_predx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix for XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the confusion matrix for XGB\n",
    "cm = confusion_matrix(y_test, y_test_predrf)\n",
    "\n",
    "# Creating a ConfusionMatrixDisplay object\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=XGB_model.classes_)\n",
    "\n",
    "# Plotting the confusion matrix\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix for XGBosst model', y=1.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = XGB_model.predict_proba(X_test)[::,1]\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test, y_pred_proba)\n",
    "auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
    "plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic Curve')\n",
    "plt.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dumping XGBoost Classifier\n",
    "joblib.dump(XGB_model, 'XGB_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Dictionary of models \n",
    "models = {\n",
    "    \"Decision Tree\": DTmodel,  \n",
    "    \"Random Forest\": rf_model, \n",
    "    \"Logistic Regression\": lrmodel,    \n",
    "    \"KNN\": knn_model, \n",
    "    \"XGBoost\": XGB_model  \n",
    "}\n",
    "\n",
    "# Training and evaluating each model\n",
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, \"predict_proba\") else None\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_pred_proba) if y_pred_proba is not None else None\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    results.append({\n",
    "        \"Model\": name,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"AUC\": auc,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1 Score\": f1\n",
    "    })\n",
    "\n",
    "# Displaying results for each model\n",
    "import pandas as pd\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost performs best overall, with the highest scores in accuracy, precision, F1 ,and AUC while also having a strong Recall"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 5890022,
     "sourceId": 9644700,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
